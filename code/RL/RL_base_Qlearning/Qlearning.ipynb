{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习之Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q-learning算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialize $Q(s,a)$ arbitrarily  \n",
    "> Repeat (for each episode):  \n",
    "> &ensp;&ensp; Initialize $s$  \n",
    "> &ensp;&ensp; Repeat (for each step of episode):  \n",
    "> &ensp;&ensp;&ensp;&ensp; Choose $a$ from $s$ using policy derived from $Q(e.g.,\\epsilon-greedy)$  \n",
    "> &ensp;&ensp;&ensp;&ensp; Take action $a$, observe $r$, $s'$  \n",
    "> &ensp;&ensp;&ensp;&ensp; $Q(s,a) \\leftarrow Q(s,a) + \\alpha*[r + \\gamma*max_{a'}Q(s',a') - Q(s,a)]$  \n",
    "> &ensp;&ensp;&ensp;&ensp; $s \\leftarrow s'$;  \n",
    "> &ensp;&ensp; until $s$ is terminal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Q-learning算法说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在Q-learning算法中，$s$表示智能体agent所处的环境状态，$a$为智能体所采取的动作, 智能体采取动作之后会获得环境的反馈$r$，以及观察到新的环境状态$s'$。$\\alpha$为迭代步长。$\\gamma$是折扣因子，取值范围为\\[0, 1\\)，表达了对长远未来回报的考虑程度，当$\\gamma$=0时，只顾眼前的回报。$Q(e.g.,\\epsilon-greedy)$ 为选择动作时的$\\epsilon$贪婪策略,当$rand()<\\epsilon$时，选择具有最大$Q(s,a)$值的动作$a$。    \n",
    "> $Q(s,a)$为估计值，$\\gamma*max_{a'}Q(s',a')$为的实际值，实际上，$\\gamma*max_{a'}Q(s',a')$也为估计值,很奇妙吧！"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agent使用Q-learning来寻宝\n",
    "> 智能体Agent在4x4的二维表格世界里寻找到宝藏, 寻宝地图如下图  \n",
    "\n",
    "> ![宝藏地图](http://localhost:8888/tree/RL/img/寻宝图.png) \n",
    "\n",
    "> 左上角为起点，红色圆点表示智能体的位置，黄色方块表示宝藏位置，黑色方块表示陷阱位置   \n",
    "> 在如黑色方块处的回报是$r=-1$， 黄色方块的回报是$r=+1$，其他方块处的回报为$r=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 导入相应的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 定义4x4的二维格子世界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEenviroment(object):\n",
    "    def __init__(self):\n",
    "        self.n = 4\n",
    "        self.action = ['East', 'South', 'West', 'North']\n",
    "        self.terminal = {'death':[{'rol':2, 'col':1}, {'rol':2, 'col':2}],\n",
    "                         'target':[{'rol':3, 'col':2}]}\n",
    "        self.cav = None\n",
    "        self.window = None\n",
    "        \n",
    "    def get_feed_back(self, state, last_state):\n",
    "        rol = state['rol']\n",
    "        col = state['col']\n",
    "        r = 0\n",
    "        if last_state['rol'] == rol and last_state['col'] == col:\n",
    "            return -1\n",
    "        for t in self.terminal['death']:\n",
    "            if rol == t['rol'] and col == t['col']:\n",
    "                r = -1\n",
    "        for t in self.terminal['target']:\n",
    "            if rol == t['rol'] and col == t['col']:\n",
    "                r = 2\n",
    "        return r\n",
    "    \n",
    "    def is_done(self, state):\n",
    "        rol = state['rol']\n",
    "        col = state['col']\n",
    "        done = False\n",
    "        for t in self.terminal['death']:\n",
    "            if rol == t['rol'] and col == t['col']:\n",
    "                done = True\n",
    "                break\n",
    "        for t in self.terminal['target']:\n",
    "            if rol == t['rol'] and col == t['col']:\n",
    "                done = True\n",
    "                break\n",
    "        return done\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        rol = state['rol']\n",
    "        col = state['col']\n",
    "        next_state = {'rol':rol, 'col':col}\n",
    "        if action == 'East':\n",
    "            col += 1\n",
    "        elif action == 'South':\n",
    "            rol += 1\n",
    "        elif action == 'West':\n",
    "            col -= 1\n",
    "        elif action == 'North':\n",
    "            rol -= 1\n",
    "        else:\n",
    "            pass\n",
    "        if rol < 0 or rol >= self.n:\n",
    "            return next_state\n",
    "        if col < 0 or col >= self.n:\n",
    "            return next_state\n",
    "        next_state['rol'] = rol\n",
    "        next_state['col'] = col\n",
    "        return next_state\n",
    "    \n",
    "    def init_env(self):\n",
    "        self.window=tk.Tk()\n",
    "        self.window.title('Grid world')\n",
    "        self.window.geometry('300x300') #长宽\n",
    "        \n",
    "        self.cav=tk.Canvas(self.window,bg='green',height=300,width=300)\n",
    "            \n",
    "    def reset_world(self):\n",
    "        self.cav.delete()\n",
    "        x0,y0 = 50,50\n",
    "        step = 50\n",
    "        \n",
    "        for rol in range(self.n):\n",
    "            for col in range(self.n):\n",
    "                self.cav.create_rectangle(x0+col*step, y0+rol*step, x0+(col+1)*step, y0+(rol+1)*step,fill='gray')\n",
    "        \n",
    "        for t in self.terminal['death']:\n",
    "            rol = t['rol'] \n",
    "            col = t['col']\n",
    "            self.cav.create_rectangle(x0+col*step, y0+rol*step, x0+(col+1)*step, y0+(rol+1)*step,fill='black')\n",
    "                \n",
    "        for t in self.terminal['target']:\n",
    "            rol = t['rol'] \n",
    "            col = t['col']  \n",
    "            self.cav.create_rectangle(x0+col*step, y0+rol*step, x0+(col+1)*step, y0+(rol+1)*step,fill='yellow')\n",
    "        \n",
    "        rol = 0\n",
    "        col = 0\n",
    "        self.cav.create_oval(x0+col*step, y0+rol*step, x0+(col+1)*step, y0+(rol+1)*step,fill='red')\n",
    "        \n",
    "        self.cav.pack()\n",
    "        \n",
    "    def render_world(self, state):\n",
    "        self.reset_world()\n",
    "        x0,y0 = 50,50\n",
    "        step = 50\n",
    "        \n",
    "        rol = 0\n",
    "        col = 0\n",
    "        self.cav.create_rectangle(x0+col*step, y0+rol*step, x0+(col+1)*step, y0+(rol+1)*step,fill='gray')\n",
    "        \n",
    "        rol = state['rol']\n",
    "        col = state['col']\n",
    "        self.cav.create_oval(x0+col*step, y0+rol*step, x0+(col+1)*step, y0+(rol+1)*step,fill='red')\n",
    "        \n",
    "        self.cav.pack()\n",
    "        self.window.update_idletasks()\n",
    "#         time.sleep(0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 定义智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.init_state = {'rol':0, 'col':0}\n",
    "        self.current_state = {'rol':0, 'col':0}\n",
    "        self.current_action = 'East'\n",
    "        self.action = ['East', 'South', 'West', 'North']\n",
    "        self.feed_back = 0\n",
    "        self.q_value = {}\n",
    "        self.alpah = 0.1\n",
    "        self.epsilon = 0.6\n",
    "        self.gamma = 0\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.current_state = {'rol':0, 'col':0}\n",
    "        \n",
    "    def init_q_value(self, env):\n",
    "        for rol in range(env.n):\n",
    "            for col in range(env.n):\n",
    "                state = str(rol) + '-' + str(col)\n",
    "                self.q_value.update({state:{}})\n",
    "                for a in self.action:\n",
    "                    self.q_value[state].update({a:0})\n",
    "    \n",
    "    def get_action_of_max_q_value(self, state):\n",
    "        rol = state['rol']\n",
    "        col = state['col']\n",
    "        state = str(rol) + '-' + str(col)\n",
    "        action_q_value = self.q_value[state]\n",
    "        max_action = ''\n",
    "        max_q_value = -10000\n",
    "        for action, q_value in action_q_value.items():\n",
    "            if q_value > max_q_value:\n",
    "                max_action = action\n",
    "                max_q_value = q_value\n",
    "        similiar_max_action = []\n",
    "        for action, q_value in action_q_value.items():\n",
    "            if q_value == max_q_value:\n",
    "                similiar_max_action.append(action)\n",
    "        max_action = similiar_max_action[random.randint(0, len(similiar_max_action)-1)]\n",
    "        return max_action, max_q_value\n",
    "    \n",
    "    def choose_action(self):\n",
    "        max_action, _ = self.get_action_of_max_q_value(self.current_state)\n",
    "        if random.random() < self.epsilon:\n",
    "            self.current_action = max_action\n",
    "        else:\n",
    "            self.current_action = self.action[random.randint(0, len(self.action)-1)]\n",
    "    \n",
    "    def take_action(self, env):\n",
    "        next_state = env.get_next_state(self.current_state, self.current_action)\n",
    "        feed_back = env.get_feed_back(next_state, self.current_state)\n",
    "        done = env.is_done(self.current_state)\n",
    "        return next_state, feed_back, done\n",
    "    \n",
    "    def update_q_value(self, next_state,feed_back):\n",
    "        rol = self.current_state['rol']\n",
    "        col = self.current_state['col']\n",
    "        current_state = str(rol) + '-' + str(col)\n",
    "        q = self.q_value[current_state][self.current_action]\n",
    "        _, max_q = self.get_action_of_max_q_value(next_state)\n",
    "        q = q + self.alpah * (feed_back + self.gamma * max_q - q)\n",
    "        self.q_value[current_state][self.current_action] = q\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 使用Q-learning寻宝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------  第 0 回合 -----------\n",
      "---------------  第 1 回合 -----------\n",
      "---------------  第 2 回合 -----------\n",
      "---------------  第 3 回合 -----------\n",
      "---------------  第 4 回合 -----------\n",
      "---------------  第 5 回合 -----------\n",
      "---------------  第 6 回合 -----------\n",
      "---------------  第 7 回合 -----------\n",
      "---------------  第 8 回合 -----------\n",
      "---------------  第 9 回合 -----------\n",
      "---------------  第 10 回合 -----------\n",
      "---------------  第 11 回合 -----------\n",
      "---------------  第 12 回合 -----------\n",
      "---------------  第 13 回合 -----------\n",
      "---------------  第 14 回合 -----------\n",
      "---------------  第 15 回合 -----------\n",
      "---------------  第 16 回合 -----------\n",
      "---------------  第 17 回合 -----------\n",
      "---------------  第 18 回合 -----------\n",
      "---------------  第 19 回合 -----------\n",
      "{'0-0': {'East': 0.0, 'South': 8.660665053070482e-07, 'West': -0.1, 'North': -0.19}, '0-1': {'East': 0.0, 'South': 0.0, 'West': 0.0, 'North': -0.271}, '0-2': {'East': 0.0, 'South': 0.0, 'West': 0.0, 'North': -0.1}, '0-3': {'East': -0.1, 'South': 0.0, 'West': 0.0, 'North': -0.1}, '1-0': {'East': 0.0, 'South': 5.12930095186988e-05, 'West': -0.19, 'North': 0.0}, '1-1': {'East': 0.0, 'South': -0.1, 'West': 0.0, 'North': 0.0}, '1-2': {'East': 0.0, 'South': -0.19, 'West': 0.0, 'North': 0.0}, '1-3': {'East': -0.19, 'South': 0.0, 'West': 0.0, 'North': 0.0}, '2-0': {'East': -0.1, 'South': 0.002218631625857917, 'West': -0.19, 'North': 0.0}, '2-1': {'East': 0, 'South': 0.0, 'West': 0, 'North': 0.0}, '2-2': {'East': 0.0, 'South': 0.200056002, 'West': -0.1, 'North': 0.0}, '2-3': {'East': -0.1, 'South': 0.0, 'West': -0.271, 'North': 0.0}, '3-0': {'East': 0.06826247095774123, 'South': -0.1, 'West': -0.1, 'North': 0.0}, '3-1': {'East': 1.437576028293061, 'South': 0, 'West': 0, 'North': -0.1}, '3-2': {'East': 0, 'South': 0, 'West': 0.07141472962470563, 'North': -0.09799943998}, '3-3': {'East': -0.1, 'South': -0.1, 'West': 0, 'North': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = GridEenviroment()\n",
    "agent = Agent()\n",
    "\n",
    "agent.epsilon = 0.9\n",
    "agent.gamma = 0.1\n",
    "\n",
    "agent.init_q_value(env)\n",
    "env.init_env()\n",
    "\n",
    "max_episode = 20\n",
    "for episode in range(max_episode):\n",
    "    agent.reset_state()\n",
    "    print('---------------  第 %d 回合 -----------' %  episode)\n",
    "    while True:\n",
    "        env.render_world(agent.current_state)\n",
    "        agent.choose_action()\n",
    "        next_state, feed_back, done = agent.take_action(env)\n",
    "        agent.update_q_value(next_state, feed_back)\n",
    "        agent.current_state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(agent.q_value)\n",
    "\n",
    "env.window.mainloop()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 训练结束 与 最佳策略\n",
    "> 最佳策略：   \n",
    "> ![最优寻宝路线](http://localhost:8888/tree/RL/img/最优寻宝路线.jpg) \n",
    "\n",
    "\n",
    "> 智能体的学习过程：\n",
    "> <video id=\"video\" controls=\"\" preload=\"none\" poster=\"http://localhost:8888/tree/RL/img/寻宝图.png\">\n",
    "    <source id=\"mp4\" src=\"http://localhost:8888/tree/RL/img/Gridworld.mp4\" type=\"video/mp4\">\n",
    "    <p>智能体的学习过程</p>\n",
    "  </video>\n",
    "> 最终经过 10 回合后，智能体在这个环境学到了最佳行动策略，如上图，使用该策略可以很快就找到了宝藏   \n",
    "> 最佳策略由$Q(s,a)$取最最大值可得到，最终学到的$Q(s,a)$值如下：\n",
    ">      \n",
    "{   \n",
    "'0-0': {'East': 0.0, 'South': 1.7820526329856689e-07, 'West': -0.271, 'North': -0.271},    \n",
    "'0-1': {'East': 0.0, 'South': 0.0, 'West': 0.0, 'North': -0.19},    \n",
    "'0-2': {'East': 0.0, 'South': 0.0, 'West': 0.0, 'North': -0.1},    \n",
    "'0-3': {'East': -0.1, 'South': 0.0, 'West': 0.0, 'North': -0.1},    \n",
    "'1-0': {'East': 0.0, 'South': 1.7304008947845952e-05, 'West': -0.19, 'North': 0.0},    \n",
    "'1-1': {'East': 0.0, 'South': -0.1, 'West': 0.0, 'North': 0.0},     \n",
    "'1-2': {'East': 0.0, 'South': -0.34390000000000004, 'West': 0.0, 'North': 0.0},    \n",
    "'1-3': {'East': -0.19, 'South': 2.000000000000001e-05, 'West': 0.0, 'North': 0.0},    \n",
    "'2-0': {'East': -0.1, 'South': 0.0011747059721527013, 'West': -0.1, 'North': 0.0},    \n",
    "'2-1': {'East': -0.1, 'South': 0.0, 'West': 0, 'North': 0.0},    \n",
    "'2-2': {'East': 0.0, 'South': 0.2, 'West': -0.19, 'North': 0},    \n",
    "'2-3': {'East': -0.19, 'South': 0.005600200000000001, 'West': -0.1, 'North': 0.0},     \n",
    "'3-0': {'East': 0.050228083114677295, 'South': -0.19, 'West': -0.189944, 'North': 0.0},     \n",
    "'3-1': {'East': 1.2263596317505943, 'South': -0.1, 'West': 0.00029951146164151703, 'North': -0.1},     \n",
    "'3-2': {'East': 0.03328359434399456, 'South': -0.1, 'West': 0, 'North': -0.098},    \n",
    "'3-3': {'East': -0.1, 'South': -0.18619980000000003, 'West': 0.542074002, 'North': 0.0}   \n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
