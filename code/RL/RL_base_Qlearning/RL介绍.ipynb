{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL - instructing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 强化学习与监督学习、非监督学习的区别\n",
    "> 强化学习：通过与环境交互，在不断尝试，不断试错中学习规律。  \n",
    "> 监督学习：直接学习数据与数据标签的对应规律。  \n",
    "> 非监督学习：根据数据推断出数据本身内在结构上的一些规律。这类算法由聚类算法，关联规则算法。  \n",
    "> 半监督学习：数据只有部分标识，另一部分未被标识。一个基本假设就是相似数据具有相似的输出。具体分为纯半监督学习和直推学习。纯半监督学习是指假设未被标识数据并非待预测数据，直推学习是指未被标识的数据作为待预测数据并给出其标识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 强化学习的基本理论基础\n",
    "> 基本原理：与环境交互，获得反馈，根据反馈学习策略，并得到下一步动作，最终学习到最优策略。   \n",
    "> ![强化学习基本原理](http://localhost:8888/tree/RL/img/RL_intro.PNG)   \n",
    "> 基本的数学模型：MDP，马尔科夫决策过程模型，该模型由五元组来描述，${S,A,P,R,\\gamma}$，其中$S$是状态集，$A$是动作集，$P$是转移概率集，$R$是奖励值集，$\\gamma$是折扣因子。   马尔科夫决策过程假设已知当前状态，那么未来的状态不依赖过去发生的历史状态。$P_{s,s'}^{a}=P{\\{S_{t+1}=s' | S_{t}=s, A_{t}=a\\}} $。   \n",
    "> 学习的目标：找到最优策略$\\pi$，使得该策略下的累积回报$G^{\\pi}_t$最大。策略$\\pi(a|s)=P{\\{A_t=a|S_t=s\\}}$,累积回报函数 $G^{\\pi}_t=R_{t+1} + \\gamma*R_{t+2} + ... = \\sum^{\\infty}_{k=0}\\gamma^{k}R_{t+k+1}$  \n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 强化学习的算法分类\n",
    "> 1. 根据环境是否已知分为 基于模型的方法(Model-Based RL)与无模型方法(Model-Free RL)   \n",
    "> &ensp;&ensp; 环境是否已知表示转移概率和奖励值已知\n",
    "> &ensp;&ensp; Q_learning,Sarsa,Policy Gradient等方法均可以用于基于模型的方法与无模型方法，只是基于模型方法比无模型方法多了一个为现实世界建模的过程，这样建立的虚拟环境模型使智能体有了想象能力，不必与真实环境交互后才知道结果比如AlphaGo。   \n",
    "> 2. 根据决策方式的不同可分为基于概率的方法(Policy-Based RL)与基于价值的方法(Value-Based RL)   \n",
    "> &ensp;&ensp; 基于概率的方法可直接输出下一步采取的各种动作的概率，然后根据概率选择行动，每种动作都可能被选择到，只是概率大小不一样，即使概率最高的动作也不一定会被选择。对于连续动作的情况，基于概率的方法可以使用动作的概率分布来选择动作，这也是基于概率方法的优点之一。常用的方法有Policy Gradient等方法。\n",
    "> &ensp;&ensp; 基于价值的方法是输出各种动作的价值，然后选择价值最高的动作，其他动作就不会被选择了。不能用于连续动作情况。常用的Q_learning,Sarsa等方法。\n",
    "> &ensp;&ensp; 基于概率的方法与基于价值的方法可以结合形成更有效的方法Actor-Critic方法。Actor会基于概率作出动作，Critic会对于做出的动作给出价值，这样就加速了原有Policy Gradient的学习效率。   \n",
    "> 3. 根据更新方式分为回合更新的方法(Monte-Carlo update)与单步更新的方法(Temporal-Difference update)   \n",
    "> &ensp;&ensp; 回合更新是需要等到一个回合结束之后才更新已有的策略。常用的方法有基本的Policy Gradient方法，Mote-Carlo learning方法等。     \n",
    "> &ensp;&ensp; 单步更新是需要每步均可更新已有的策略。常用的方法有Q_learning，Sarsa，升级版的Policy Gradient方法等。  \n",
    "> 3. 根据是否在现场可分为在线学习方法(On-Policy) 与离线学习方法(Off-Policy)   \n",
    "> &ensp;&ensp; 在线学习是指智能体在亲自在场学习。常用的方法有Sarsa，Sarsa($\\lambda$)等方法\n",
    "> &ensp;&ensp; 离线学习方法是指智能体可不在场，可以使用别人的经历，或者先把经历存下来后面根据经历慢慢学习常用的方法有Q_learning,Deep Q Network等方法。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 强化学习的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 实验平台\n",
    "> 国外：openai  \n",
    "> 国内：百度ai   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
